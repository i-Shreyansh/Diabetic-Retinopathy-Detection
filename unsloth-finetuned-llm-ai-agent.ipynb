{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shreyanshmanavshukla/unsloth-finetuned-llm-ai-agent?scriptVersionId=256806210\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"%%time\n%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:57:52.518056Z","iopub.execute_input":"2025-08-19T07:57:52.518215Z","iopub.status.idle":"2025-08-19T07:58:06.436537Z","shell.execute_reply.started":"2025-08-19T07:57:52.518198Z","shell.execute_reply":"2025-08-19T07:58:06.435726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_tokens = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")  # secret hugging face token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:58:06.438291Z","iopub.execute_input":"2025-08-19T07:58:06.438544Z","iopub.status.idle":"2025-08-19T07:58:06.55332Z","shell.execute_reply.started":"2025-08-19T07:58:06.438517Z","shell.execute_reply":"2025-08-19T07:58:06.552646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n%%capture\n!pip install langgraph langchain-community\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:58:06.55406Z","iopub.execute_input":"2025-08-19T07:58:06.554267Z","iopub.status.idle":"2025-08-19T07:58:14.049629Z","shell.execute_reply.started":"2025-08-19T07:58:06.55424Z","shell.execute_reply":"2025-08-19T07:58:14.048845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base Model","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n\n    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n\n    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:58:23.254747Z","iopub.execute_input":"2025-08-19T07:58:23.255459Z","iopub.status.idle":"2025-08-19T07:59:18.221981Z","shell.execute_reply.started":"2025-08-19T07:58:23.255421Z","shell.execute_reply":"2025-08-19T07:59:18.221357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:18.223278Z","iopub.execute_input":"2025-08-19T07:59:18.223567Z","iopub.status.idle":"2025-08-19T07:59:24.516195Z","shell.execute_reply.started":"2025-08-19T07:59:18.22354Z","shell.execute_reply":"2025-08-19T07:59:24.515512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprcess","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:24.516908Z","iopub.execute_input":"2025-08-19T07:59:24.517148Z","iopub.status.idle":"2025-08-19T07:59:29.425067Z","shell.execute_reply.started":"2025-08-19T07:59:24.517121Z","shell.execute_reply":"2025-08-19T07:59:29.424519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:29.427078Z","iopub.execute_input":"2025-08-19T07:59:29.427297Z","iopub.status.idle":"2025-08-19T07:59:43.392494Z","shell.execute_reply.started":"2025-08-19T07:59:29.427279Z","shell.execute_reply":"2025-08-19T07:59:43.391524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[5][\"conversations\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:43.394087Z","iopub.execute_input":"2025-08-19T07:59:43.394331Z","iopub.status.idle":"2025-08-19T07:59:43.400049Z","shell.execute_reply.started":"2025-08-19T07:59:43.394304Z","shell.execute_reply":"2025-08-19T07:59:43.399355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[5][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:43.400877Z","iopub.execute_input":"2025-08-19T07:59:43.401158Z","iopub.status.idle":"2025-08-19T07:59:45.474209Z","shell.execute_reply.started":"2025-08-19T07:59:43.401135Z","shell.execute_reply":"2025-08-19T07:59:45.473421Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine Tune","metadata":{}},{"cell_type":"code","source":"from trl import SFTConfig, SFTTrainer\nfrom transformers import DataCollatorForSeq2Seq\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    packing = False, # Can make training 5x faster for short sequences.\n    args = SFTConfig(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T07:59:45.475149Z","iopub.execute_input":"2025-08-19T07:59:45.475475Z","iopub.status.idle":"2025-08-19T08:01:08.342491Z","shell.execute_reply.started":"2025-08-19T07:59:45.475455Z","shell.execute_reply":"2025-08-19T08:01:08.34184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:01:08.343403Z","iopub.execute_input":"2025-08-19T08:01:08.343656Z","iopub.status.idle":"2025-08-19T08:01:32.542593Z","shell.execute_reply.started":"2025-08-19T08:01:08.34362Z","shell.execute_reply":"2025-08-19T08:01:32.541941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:01:32.543574Z","iopub.execute_input":"2025-08-19T08:01:32.543857Z","iopub.status.idle":"2025-08-19T08:01:32.550879Z","shell.execute_reply.started":"2025-08-19T08:01:32.543822Z","shell.execute_reply":"2025-08-19T08:01:32.550185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:01:32.552927Z","iopub.execute_input":"2025-08-19T08:01:32.553124Z","iopub.status.idle":"2025-08-19T08:01:35.063737Z","shell.execute_reply.started":"2025-08-19T08:01:32.553109Z","shell.execute_reply":"2025-08-19T08:01:35.062731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n                         temperature = 1.5, min_p = 0.1)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:01:35.064732Z","iopub.execute_input":"2025-08-19T08:01:35.065149Z","iopub.status.idle":"2025-08-19T08:01:42.346106Z","shell.execute_reply.started":"2025-08-19T08:01:35.065105Z","shell.execute_reply":"2025-08-19T08:01:42.345486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:03:20.821606Z","iopub.execute_input":"2025-08-19T08:03:20.821911Z","iopub.status.idle":"2025-08-19T08:03:21.485682Z","shell.execute_reply.started":"2025-08-19T08:03:20.821873Z","shell.execute_reply":"2025-08-19T08:03:21.485068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge to 16bit\nif False: model.save_pretrained_merged(\"model_16bit\", tokenizer, save_method = \"merged_16bit\",)\nif False: model.push_to_hub_merged(\"ShuklaShreyansh/LoraModel_16bit\", tokenizer, save_method = \"merged_16bit\", token = hf_tokens)\n\n# Merge to 4bit\nif False: model.save_pretrained_merged(\"model_4bit\", tokenizer, save_method = \"merged_4bit_forced\",)\nif False: model.push_to_hub_merged(\"ShuklaShreyansh/LoraModel_4bit\", tokenizer, save_method = \"merged_4bit\", token = hf_tokens)\n\n# Just LoRA adapters\nif False:\n    model.save_pretrained(\"model\")\n    tokenizer.save_pretrained(\"model\")\nif False:\n    model.push_to_hub(\"ShuklaShreyansh/LoraModel\", token = hf_tokens)\n    tokenizer.push_to_hub(\"ShuklaShreyansh/LoraModel\", token = hf_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:28:13.066585Z","iopub.execute_input":"2025-08-19T06:28:13.067305Z","iopub.status.idle":"2025-08-19T06:31:33.722888Z","shell.execute_reply.started":"2025-08-19T06:28:13.067271Z","shell.execute_reply":"2025-08-19T06:31:33.722194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"if True:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"/kaggle/working/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n                   use_cache = True, temperature = 1.5, min_p = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:04:23.036777Z","iopub.execute_input":"2025-08-19T08:04:23.037439Z","iopub.status.idle":"2025-08-19T08:04:37.306841Z","shell.execute_reply.started":"2025-08-19T08:04:23.03741Z","shell.execute_reply":"2025-08-19T08:04:37.306275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T19:29:14.233259Z","iopub.execute_input":"2025-08-18T19:29:14.233563Z","iopub.status.idle":"2025-08-18T19:29:14.358482Z","shell.execute_reply.started":"2025-08-18T19:29:14.233539Z","shell.execute_reply":"2025-08-18T19:29:14.357672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AI Agent","metadata":{}},{"cell_type":"code","source":"# From Hugging Face\nif False:\n    from unsloth import FastLanguageModel\n    import torch\n    \n    model_name = 'ShuklaShreyansh/LoraModel_16bit'\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=model_name,  # your Hugging Face repo\n        max_seq_length=2048,\n        dtype=None,          # Auto-detect FP16/BF16\n        load_in_4bit=False,   # saves memory, optional\n        token = hf_tokens\n    )\n    \n    FastLanguageModel.for_inference(model)\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:06:36.783118Z","iopub.execute_input":"2025-08-19T08:06:36.783706Z","iopub.status.idle":"2025-08-19T08:06:36.78839Z","shell.execute_reply.started":"2025-08-19T08:06:36.783683Z","shell.execute_reply":"2025-08-19T08:06:36.787631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional speed boost\nmodel = torch.compile(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:07:20.299343Z","iopub.execute_input":"2025-08-19T08:07:20.299627Z","iopub.status.idle":"2025-08-19T08:07:20.305589Z","shell.execute_reply.started":"2025-08-19T08:07:20.299605Z","shell.execute_reply":"2025-08-19T08:07:20.305033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nfrom langchain_community.llms import HuggingFacePipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=256,\n    temperature=0.7,\n    top_p=0.9,\n)\n\nllm = HuggingFacePipeline(pipeline=pipe)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:07:24.723197Z","iopub.execute_input":"2025-08-19T08:07:24.72345Z","iopub.status.idle":"2025-08-19T08:07:25.207006Z","shell.execute_reply.started":"2025-08-19T08:07:24.723434Z","shell.execute_reply":"2025-08-19T08:07:25.206286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Workflow","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import StateGraph, END\n\n# Define the state\nclass ChatState(dict):\n    user_input: str\n    model_output: str\n\n# Create the graph\ngraph = StateGraph(ChatState)\n\n# Node: Use your LLM\ndef llm_node(state: ChatState):\n    response = llm(state[\"user_input\"])\n    return {\"model_output\": response}\n\n# Add nodes\ngraph.add_node(\"LLM\", llm_node)\n\n# Define flow\ngraph.set_entry_point(\"LLM\")\ngraph.add_edge(\"LLM\", END)\n\n# Compile graph\napp = graph.compile()\napp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:07:35.575588Z","iopub.execute_input":"2025-08-19T08:07:35.575869Z","iopub.status.idle":"2025-08-19T08:07:35.885189Z","shell.execute_reply.started":"2025-08-19T08:07:35.575847Z","shell.execute_reply":"2025-08-19T08:07:35.884478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state = {\"user_input\": \"Explain crop rotation in simple words.\"}\nresult = app.invoke(state)\n\nprint(result[\"model_output\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:07:41.330778Z","iopub.execute_input":"2025-08-19T08:07:41.331457Z","iopub.status.idle":"2025-08-19T08:07:56.94474Z","shell.execute_reply.started":"2025-08-19T08:07:41.331433Z","shell.execute_reply":"2025-08-19T08:07:56.943944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For more information\ncheckout   [Unsloth Github](https://github.com/unslothai/unsloth)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}